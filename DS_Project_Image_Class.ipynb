{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h47rDu04ybob"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT-pJtjcJ3dD"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "realFakeCNN = Sequential()\n",
        "\n",
        "# Add the first Convolutional layer with 32 filters, 3x3 size, and input shape (200, 200, 3)\n",
        "realFakeCNN.add(Conv2D(32, (3, 3), input_shape=(200, 200, 3)))\n",
        "realFakeCNN.add(Activation('relu'))\n",
        "realFakeCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add the second Convolutional layer with 32 filters, 3x3 size\n",
        "realFakeCNN.add(Conv2D(32, (3, 3)))\n",
        "realFakeCNN.add(Activation('relu'))\n",
        "realFakeCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add the third Convolutional layer with 32 filters, 3x3 size\n",
        "realFakeCNN.add(Conv2D(32, (3, 3)))\n",
        "realFakeCNN.add(Activation('relu'))\n",
        "realFakeCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add the fourth Convolutional layer with 32 filters, 3x3 size\n",
        "realFakeCNN.add(Conv2D(32, (3, 3)))\n",
        "realFakeCNN.add(Activation('relu'))\n",
        "realFakeCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the dataset\n",
        "realFakeCNN.add(Flatten())\n",
        "\n",
        "# Add a Dense layer with 64 units and ReLU activation\n",
        "realFakeCNN.add(Dense(64))\n",
        "realFakeCNN.add(Activation('relu'))\n",
        "\n",
        "# Add Dropout layer to overcome overfitting\n",
        "realFakeCNN.add(Dropout(0.5))\n",
        "\n",
        "# Add one more fully connected layer to get the output in n-dimensional classes (a vector will be the output)\n",
        "realFakeCNN.add(Dense(1))\n",
        "\n",
        "# Add the Sigmoid function to convert to probabilities\n",
        "realFakeCNN.add(Activation('sigmoid'))\n",
        "\n",
        "# Print a summary of the network\n",
        "realFakeCNN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAm4JDrNJ3MI"
      },
      "outputs": [],
      "source": [
        "# Compile the model with RMSprop optimizer, binary crossentropy loss, and accuracy metric\n",
        "realFakeCNN.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_train_test(source_dir, train_dir, test_dir, split_ratio=0.7):\n",
        "    # List all files in the source directory\n",
        "    files = os.listdir(source_dir)\n",
        "    num_files = len(files)\n",
        "    \n",
        "    # Calculate the number of files for the train set\n",
        "    num_train = int(num_files * split_ratio)\n",
        "    \n",
        "    # Shuffle the files randomly\n",
        "    random.shuffle(files)\n",
        "    \n",
        "    # Create train and test directories if they don't exist\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    \n",
        "    # Move files to train and test directories based on the split ratio\n",
        "    for i, file in enumerate(files):\n",
        "        source_file = os.path.join(source_dir, file)\n",
        "        if i < num_train:\n",
        "            shutil.copy(source_file, os.path.join(train_dir, file))\n",
        "        else:\n",
        "            shutil.copy(source_file, os.path.join(test_dir, file))\n",
        "\n",
        "# Replace these paths with your source directory, train directory, and test directory\n",
        "source_directory = ''\n",
        "train_directory = ''\n",
        "test_directory = ''\n",
        "\n",
        "# Split the contents of the source directory into train and test\n",
        "split_train_test(source_directory, train_directory, test_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the paths to the main folder and subfolders\n",
        "main_folder = ''\n",
        "subfolder = ''\n",
        "\n",
        "# Function to move files from a source folder to the main folder\n",
        "def move_files(source_folder):\n",
        "    for file_name in os.listdir(source_folder):\n",
        "        source_path = os.path.join(source_folder, file_name)\n",
        "        destination_path = os.path.join(main_folder, file_name)\n",
        "\n",
        "        # Check if source and destination paths are the same\n",
        "        if source_path != destination_path:\n",
        "            shutil.move(source_path, destination_path)\n",
        "\n",
        "# Move files from each subfolder to the main folder\n",
        "move_files(subfolder)\n",
        "\n",
        "print(\"Folders combined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def move_files_from_subdirectories(root_directory, destination_directory, start_index, end_index, step=1000):\n",
        "    try:\n",
        "        # Iterate over the specified range of indices with the given step\n",
        "        for index in range(start_index, end_index + 1, step):\n",
        "            subdirectory_name = f\"{index:06d}\"\n",
        "            subdirectory_path = os.path.join(root_directory, subdirectory_name)\n",
        "\n",
        "            # Check if the subdirectory exists\n",
        "            if os.path.isdir(subdirectory_path):\n",
        "                # Define the source directory within the subdirectory\n",
        "                source_directory = os.path.join(subdirectory_path, '')  # Adjust the subdirectory as needed\n",
        "\n",
        "                # Call the function to move files from source to destination\n",
        "                move_files_to_destination(source_directory, destination_directory)\n",
        "\n",
        "        print(\"All files moved successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Root directory not found: {root_directory}\")\n",
        "\n",
        "# Function to move files from source to destination\n",
        "def move_files_to_destination(source_folder, destination_folder):\n",
        "    try:\n",
        "        # Ensure the destination folder exists, create it if not\n",
        "        os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "        # Iterate over files in the source folder\n",
        "        for file_name in os.listdir(source_folder):\n",
        "            source_path = os.path.join(source_folder, file_name)\n",
        "            destination_path = os.path.join(destination_folder, file_name)\n",
        "\n",
        "            # Check if the file exists before moving\n",
        "            if os.path.isfile(source_path):\n",
        "                # Print source and destination paths\n",
        "                print(f\"Moving: {source_path} to {destination_path}\")\n",
        "\n",
        "                # Move the file\n",
        "                shutil.move(source_path, destination_path)\n",
        "            else:\n",
        "                print(f\"File not found: {source_path}\")\n",
        "\n",
        "        print(\"Files moved successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Source directory not found: {source_folder}\")\n",
        "\n",
        "# Example usage:\n",
        "root_directory = ''  # Specify the root directory\n",
        "destination_directory = ''  # Specify the destination directory\n",
        "start_index = 00000  # Specify the start index\n",
        "end_index = 99000  # Specify the end index\n",
        "\n",
        "# Call the function to move files from subdirectories to the destination\n",
        "move_files_from_subdirectories(root_directory, destination_directory, start_index, end_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.25,\n",
        "    zoom_range=0.25,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data normalization for testing set\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the training data with data augmentation\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    '/Users/jamessweat/Desktop/ml-project/OURDATA/first_test/train',\n",
        "    target_size=(200, 200),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the testing data without data augmentation\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    '/Users/jamessweat/Desktop/ml-project/OURDATA/first_test/test',\n",
        "    target_size=(200, 200),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model using the training set and validate on the test set\n",
        "realFakeCNN.fit(\n",
        "    training_set,\n",
        "    steps_per_epoch=len(training_set),\n",
        "    epochs=10,\n",
        "    validation_data=test_set,\n",
        "    validation_steps=len(test_set)\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "# realFakeCNN.save('realfake_cnn_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained VGG16 model without classifier/fully-connected layers\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "\n",
        "# Freeze the layers \n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Add new classifier layers\n",
        "x = Flatten()(vgg16.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions2 = Dense(1, activation='sigmoid')(x) \n",
        "\n",
        "# Create the model \n",
        "realFakeCNN = tf.keras.Model(inputs=vgg16.input, outputs=predictions2)\n",
        "\n",
        "# Compile model\n",
        "realFakeCNN.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model on real vs AI-generated images dataset \n",
        "realFakeCNN.fit(training_set, epochs=10, validation_data=(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load_model('realfake_cnn_model_2')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluation = model.evaluate(test_set)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "foundations_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "130ab6afa2160780407dd824f8707411efe2944e80147d5d0e68e0f3551ba63a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
